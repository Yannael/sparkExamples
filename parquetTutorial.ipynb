{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CSV in Spark and save as Parquet, from standard or HDFS filesystems\n",
    "\n",
    "This notebook show how to load a CSV file with Spark, and store the data as Parquet. This is fine to save small datasets. If 'large' datasets (e.g. 10s of MB), store data on HDFS and use Impala to import. See http://mlg.ulb.ac.be/w/Impala"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session\n",
    "\n",
    "Note: Spark should be installed, with binaries available from PATH, and PYTHONPATH set. See\n",
    "* http://mlg.ulb.ac.be/w/Installation:_Spark_standalone_and_IPython\n",
    "* http://mlg.ulb.ac.be/w/Get_started:_Jupyter_notebook_and_Spark_UI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] =\"--conf spark.driver.memory=2g  pyspark-shell\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Start Spark session with local master and 2 cores\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"parquetTutorial\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load file and store as parquet\n",
    "\n",
    "data.csv contains:\n",
    "\n",
    "```\n",
    " 0,3375563642,POINT (4.991196 51.11461),2016-10-10 02:00:17\n",
    " 1,3369963219,POINT (3.991617 50.68576),2016-10-10 02:00:02\n",
    " 2,74234088,POINT (5.871564 50.54401),2016-10-10 02:00:11\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: long (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Load from local folder, you have to give complete path preceded by file:///\n",
    "filename=\"file:///home/yleborgn/mobiaid/data.csv\"\n",
    "df = spark.read.load(filename, \n",
    "                          format='com.databricks.spark.csv', \n",
    "                          header='false', \n",
    "                          inferSchema='true')\n",
    "\n",
    "df.printSchema() #Note: types properly inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, _c1=3375563642, _c2=u'POINT (4.991196 51.11461)', _c3=datetime.datetime(2016, 10, 10, 2, 0, 17)),\n",
       " Row(_c0=1, _c1=3369963219, _c2=u'POINT (3.991617 50.68576)', _c3=datetime.datetime(2016, 10, 10, 2, 0, 2)),\n",
       " Row(_c0=2, _c1=74234088, _c2=u'POINT (5.871564 50.54401)', _c3=datetime.datetime(2016, 10, 10, 2, 0, 11))]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save to Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.13 ms, sys: 271 Âµs, total: 2.4 ms\n",
      "Wall time: 524 ms\n"
     ]
    }
   ],
   "source": [
    "filename=\"file:///home/yleborgn/mobiaid/dataframe.parquet\"\n",
    "%time df.write.mode(\"overwrite\").parquet(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interact with HDFS\n",
    "\n",
    "HDFS masternode is at hdfs://master01:8020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.07 ms, sys: 0 ns, total: 3.07 ms\n",
      "Wall time: 841 ms\n"
     ]
    }
   ],
   "source": [
    "#This writes to HDFS\n",
    "filename=\"hdfs://master01:8020/user/yleborgn/dataframe.parquet\"\n",
    "%time df.write.mode(\"overwrite\").parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 3.36 ms, total: 3.36 ms\n",
      "Wall time: 231 ms\n"
     ]
    }
   ],
   "source": [
    "#This reads from HDFS Impala/Hive tables:\n",
    "filename=\"hdfs://master01:8020/user/hive/warehouse/mlg.db/tab_parquet\"\n",
    "%time df=spark.read.parquet(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(gid=0, id=2147483647, geom=bytearray(b'POINT (4.991196 51.11461)'), date_var=datetime.datetime(2016, 10, 10, 4, 0, 17)),\n",
       " Row(gid=1, id=2147483647, geom=bytearray(b'POINT (3.991617 50.68576)'), date_var=datetime.datetime(2016, 10, 10, 4, 0, 2)),\n",
       " Row(gid=2, id=74234088, geom=bytearray(b'POINT (5.871564 50.54401)'), date_var=datetime.datetime(2016, 10, 10, 4, 0, 11)),\n",
       " Row(gid=None, id=None, geom=bytearray(b''), date_var=None)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
